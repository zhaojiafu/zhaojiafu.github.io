<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <link rel="canonical" href="https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/">
    
    
    <title>python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求） | 赵家富的博客 | 保持好奇心</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="scrapy">
    <meta name="description" content="说明：今天主要学习一下翻页的功能，手动翻页的效果，前面的基础操作这里不不再依次讲解截图说明了，如果不太懂，可以参考我的上一篇scrapy博客：https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_42081389&#x2F;article&#x2F;details&#x2F;102390279 我的配置：1windows10系统2python3.6 目标网站：http:&#x2F;&#x2F;lab.scrapyd.cn&#x2F;因为这个网站是get">
<meta name="keywords" content="scrapy">
<meta property="og:type" content="article">
<meta property="og:title" content="python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）">
<meta property="og:url" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;2019&#x2F;10&#x2F;09&#x2F;b6288208a9fabc0a558dbcfc23223334&#x2F;index.html">
<meta property="og:site_name" content="赵家富的博客">
<meta property="og:description" content="说明：今天主要学习一下翻页的功能，手动翻页的效果，前面的基础操作这里不不再依次讲解截图说明了，如果不太懂，可以参考我的上一篇scrapy博客：https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_42081389&#x2F;article&#x2F;details&#x2F;102390279 我的配置：1windows10系统2python3.6 目标网站：http:&#x2F;&#x2F;lab.scrapyd.cn&#x2F;因为这个网站是get">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009104639747.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009104401174.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009112148162.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009141336984.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009141550860.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009142539895.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009144127552.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009144411822.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009144439116.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009143343474.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;2019100914554922.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;2019100915005359.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009150136151.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009150204964.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009150314529.png">
<meta property="og:updated_time" content="2019-12-02T07:00:37.349Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;zhaojiafu.github.io&#x2F;imgs&#x2F;20191009104639747.png">
    
    <link rel="shortcut icon" href="/img/cb2nm-5l9w0-001.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>


    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" target="_blank" rel="noopener" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/zhao.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">赵家富</h5>
          <a href="mailto:530519174@qq.com" target="_blank" rel="noopener" title="530519174@qq.com" class="mail">530519174@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                archives(所有)
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags/"  >
                <i class="icon icon-lg icon-tags"></i>
                tag(标签)
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories/"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories(分类)
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/zhaojiafu" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://blog.csdn.net/weixin_42081389" target="_blank" >
                <i class="icon icon-lg icon-csdn"></i>
                Csdn
              </a>
            </li>
        
      </ul>
    </div>
  </div>

</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" target="_blank" rel="noopener" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>

        <div class="flex-col header-title ellipsis">python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" target="_blank" rel="noopener" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" target="_blank" rel="noopener" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" target="_blank" rel="noopener" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">

        <h1 class="title">python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）
        </h1>
        <h5 class="subtitle">
            
                <time datetime="2019-10-09T03:34:32.000Z" itemprop="datePublished" class="page-time">
  2019-10-09
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/scrapy/">scrapy</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#说明："><span class="post-toc-number">1.</span> <span class="post-toc-text">说明：</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#我的配置："><span class="post-toc-number">1.1.</span> <span class="post-toc-text">我的配置：</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#目标网站："><span class="post-toc-number">1.2.</span> <span class="post-toc-text">目标网站：</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#今天爬虫（手动提取url，发送get请求）"><span class="post-toc-number">2.</span> <span class="post-toc-text">今天爬虫（手动提取url，发送get请求）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1、创建项目-初始化爬虫文件："><span class="post-toc-number">2.1.</span> <span class="post-toc-text">1、创建项目+初始化爬虫文件：</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2、在setting中配置"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">2、在setting中配置</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3、修改items-py："><span class="post-toc-number">2.3.</span> <span class="post-toc-text">3、修改items.py：</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4、修改爬虫程序：spiders-scrapyd-py"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">4、修改爬虫程序：spiders/scrapyd.py</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#①、scrapy-Request"><span class="post-toc-number">2.4.1.</span> <span class="post-toc-text">①、scrapy.Request()</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#②、直接上我的代码："><span class="post-toc-number">2.4.2.</span> <span class="post-toc-text">②、直接上我的代码：</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5、管道处理（一般都在这里进行数据清洗和数据储存操作）：pipelines-py"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">5、管道处理（一般都在这里进行数据清洗和数据储存操作）：pipelines.py</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1、测试spider是什么："><span class="post-toc-number">2.5.1.</span> <span class="post-toc-text">1、测试spider是什么：</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2、保存到MongoDB数据库："><span class="post-toc-number">2.5.2.</span> <span class="post-toc-text">2、保存到MongoDB数据库：</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6、我刚学scrapy对itmes模块的一个疑问："><span class="post-toc-number">2.6.</span> <span class="post-toc-text">6、我刚学scrapy对itmes模块的一个疑问：</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1、我改为item"><span class="post-toc-number">2.6.1.</span> <span class="post-toc-text">1、我改为item={}</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2、我改为：item-ScrapydCnItem"><span class="post-toc-number">2.6.2.</span> <span class="post-toc-text">2、我改为：item = ScrapydCnItem()</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3、对比，得出我认为的结论："><span class="post-toc-number">2.6.3.</span> <span class="post-toc-text">3、对比，得出我认为的结论：</span></a></li></ol></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-b6288208a9fabc0a558dbcfc23223334"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-10-09 11:34:32" datetime="2019-10-09T03:34:32.000Z"  itemprop="datePublished">2019-10-09</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/scrapy/">scrapy</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h1><p>今天主要学习一下翻页的功能，手动翻页的效果，前面的基础操作这里不不再依次讲解截图说明了，如果不太懂，可以参考我的上一篇scrapy博客：<br><a href="https://blog.csdn.net/weixin_42081389/article/details/102390279" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42081389/article/details/102390279</a></p>
<h2 id="我的配置："><a href="#我的配置：" class="headerlink" title="我的配置："></a>我的配置：</h2><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">windows10系统</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">python3.<span class="number">6</span></span></pre></td></tr></table></figure>
<h2 id="目标网站："><a href="#目标网站：" class="headerlink" title="目标网站："></a>目标网站：</h2><p><a href="http://lab.scrapyd.cn/" target="_blank" rel="noopener">http://lab.scrapyd.cn/</a><br>因为这个网站是get，响应的页面就能找到下一页的url，我记得之前测试翻页是用的腾讯招聘网站，但是现在腾讯招聘的页面改成异步获取的json数据了，如果真的爬取，我觉得直接使用requests模块比较方便，如果用scrapy，可以直接把开始的start_urls列表换成一个异步url列表集，我感觉使用scrapy那样爬取多此一举了。</p>
<p>比如这样，url列表集：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">start_urls = [<span class="string">'https://careers.tencent.com/tencentcareer/api/post/Query?timestamp=1570587185160&amp;countryId=&amp;cityId=&amp;bgIds=&amp;productId=&amp;categoryId=&amp;parentCategoryId=&amp;attrId=&amp;keyword=&amp;pageIndex=&#123;&#125;&amp;pageSize=10&amp;language=zh-cn&amp;area=cn'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">101</span>)]</span></pre></td></tr></table></figure>

<p>不过这样的我不写，这样的和我的第一个博客没有什么区别，只不过初始化的url列表数量多了而已。</p>
<h1 id="今天爬虫（手动提取url，发送get请求）"><a href="#今天爬虫（手动提取url，发送get请求）" class="headerlink" title="今天爬虫（手动提取url，发送get请求）"></a>今天爬虫（手动提取url，发送get请求）</h1><h2 id="1、创建项目-初始化爬虫文件："><a href="#1、创建项目-初始化爬虫文件：" class="headerlink" title="1、创建项目+初始化爬虫文件："></a>1、创建项目+初始化爬虫文件：</h2><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">scrapy startpoject scrapyd_cn</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">cd scrapyd_cn</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">scapy genspider scrapyd lab.scrapyd.cn/</span></pre></td></tr></table></figure>


<p>生成文件：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009104639747.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>



<h2 id="2、在setting中配置"><a href="#2、在setting中配置" class="headerlink" title="2、在setting中配置"></a>2、在setting中配置</h2><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">USER_AGENT = <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = False</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">_LEVEL = <span class="string">'DEBUG'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">LOG_LEVEL = <span class="string">"WARNING"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">   <span class="string">'scrapyd_cn.pipelines.ScrapydCnPipeline'</span>: <span class="number">300</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<h2 id="3、修改items-py："><a href="#3、修改items-py：" class="headerlink" title="3、修改items.py："></a>3、修改items.py：</h2><p>这里我们只要三维数据，需要三个字段即可。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009104401174.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>

<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">import scrapy</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">class ScrapydCnItem(scrapy.Item):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="comment"># define the fields for your item here like:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    text = scrapy.Field()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    author = scrapy.Field()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    url = scrapy.Field()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="comment"># pass</span></span></pre></td></tr></table></figure>

<h2 id="4、修改爬虫程序：spiders-scrapyd-py"><a href="#4、修改爬虫程序：spiders-scrapyd-py" class="headerlink" title="4、修改爬虫程序：spiders/scrapyd.py"></a>4、修改爬虫程序：spiders/scrapyd.py</h2><h3 id="①、scrapy-Request"><a href="#①、scrapy-Request" class="headerlink" title="①、scrapy.Request()"></a>①、scrapy.Request()</h3><p>这个只是和之前比这多了一个翻页功能，这个scrapy.Request()<br>里面有俩个必须要传递的参数，一个是url,一个是返回的函数，这里的parse是本身的方法中，继续处理数据直至页面结束，可以自己写多个方法，根据项目和网站需要定义方法，和需要的返回方法中。</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 翻页</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">            yield scrapy.Request(url=next_page, callback=self.parse)</span></pre></td></tr></table></figure>

<p>其中还有几个常用的参数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009112148162.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>
<p>scrapy.Request()中几个常用参数解释：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">url：必填：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">callback：必填：请求之后返回到方法中处理然后数据</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">enthod:默认是get,如果是post需要自己手动更改，我这里是get,忽略了。</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">cookies：字典类型，有些网站需要cookies，可以携带上单个的cookies。</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">headers：字典类型，请求头，我的这个在setting里面加入了user-agent，这里不加也可以。</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">meta：这个很常用，这里是方法之间传递参数的</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">dont_filter:是否开启过滤，默认关闭，开启之后爬取过的url,下一次不会再爬取</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">errback：和callback类似，但是是处理对应的请求url报错时会进入errback，可以进入将报错的url打印出来或者单独保留下来，后续手动测试查找报错原因</span></pre></td></tr></table></figure>
<h3 id="②、直接上我的代码："><a href="#②、直接上我的代码：" class="headerlink" title="②、直接上我的代码："></a>②、直接上我的代码：</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">import scrapy</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">from scrapyd_cn.items import ScrapydCnItem</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">class ScrapydSpider(scrapy.Spider):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    name = <span class="string">'scrapyd'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    allowed_domains = [<span class="string">'lab.scrapyd.cn'</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    start_urls = [<span class="string">'http://lab.scrapyd.cn//'</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    def parse(self, response):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 1、提取每一页的数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        div_list = response.xpath(<span class="string">'//*[@id="main"]/div[@class="quote post"]'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            <span class="comment"># extract_first() 和 get() 返回的结果是一样的。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">            text = div.xpath(<span class="string">'./span[@class="text"]/text()'</span>).get()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">            <span class="comment"># author = div.xpath('.//*[@class="author"]/text()').extract_first()</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">            author = div.xpath(<span class="string">'.//*[@class="author"]/text()'</span>).get()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">            url = div.xpath(<span class="string">'.//a[contains(text(),"详情")]/@href'</span>).get()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">            <span class="comment"># print("div", text, author, url)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">            item = ScrapydCnItem()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">            item[<span class="string">'text'</span>] = text</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">            item[<span class="string">'author'</span>] = author</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">            item[<span class="string">'url'</span>] = url</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">            yield item</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 2、下一页</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        next_page = response.xpath(<span class="string">'//*[@id="main"]//li[@class="next"]/a/@href'</span>).get()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        print(<span class="string">"下一页"</span>,next_page)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> next_page and len(next_page) &gt; <span class="number">5</span>:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">            <span class="comment"># 翻页</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">            yield scrapy.Request(url=next_page, callback=self.parse)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span>:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">            print(<span class="string">"没有下一页了"</span>*<span class="number">10</span>)</span></pre></td></tr></table></figure>


<h2 id="5、管道处理（一般都在这里进行数据清洗和数据储存操作）：pipelines-py"><a href="#5、管道处理（一般都在这里进行数据清洗和数据储存操作）：pipelines-py" class="headerlink" title="5、管道处理（一般都在这里进行数据清洗和数据储存操作）：pipelines.py"></a>5、管道处理（一般都在这里进行数据清洗和数据储存操作）：pipelines.py</h2><h3 id="1、测试spider是什么："><a href="#1、测试spider是什么：" class="headerlink" title="1、测试spider是什么："></a>1、测试spider是什么：</h3><p>下面是我测试的方法，之前使用过name属性进行过对不同的数据进行清洗和储存。你也可以自己研究测试。</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">class ScrapydCnPipeline(object):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    def process_item(self, item, spider):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># print("pipelines.py",item,spider)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 1、测试spider是什么?,结果发现spdier就是我们爬虫程序，因为一个项目里面可以有多个爬虫程序，</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># print("pipelines.py",dir(spider)) # 打印出spider含有的属性和方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># ['allowed_domains', 'close', 'crawler', 'custom_settings', 'from_crawler', 'handles_request', 'log', 'logger', 'make_requests_from_url', 'name', 'parse', 'set_crawler', 'settings', 'start_requests', 'start_urls', 'update_settings']</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># print("pipelines.py", spider.name)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 发现我的爬虫程序的name是唯一的，如果一个项目有多个爬虫程序时，可以根据name进行分别进行建立不同的方法处理（比如不同的爬虫数据，需要存入不同的数据库中，或者需要的字段还不一致）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># pipelines.py scrapyd</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 2、处理返回的数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> item</span></pre></td></tr></table></figure>

<h3 id="2、保存到MongoDB数据库："><a href="#2、保存到MongoDB数据库：" class="headerlink" title="2、保存到MongoDB数据库："></a>2、保存到MongoDB数据库：</h3><p>代码：</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">from pymongo import MongoClient</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">class ScrapydCnPipeline(object):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    def open_spider(self, spider):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        print(<span class="string">"准备创建一个数据库"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 这个会在项目开始时第一次进入pipelines.py进入，之后不再进入</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 建立于MongoClient 的连接：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        self.client = MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 得到数据库</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        self.db = self.client[<span class="string">'111_test_database_scrapyd_cn'</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 得到一个集合</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        self.collection = self.db[<span class="string">'111_test_collection_scrapyd_cn'</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">    def close_spider(self, spider):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        print(<span class="string">'项目结束，断开数据库连接'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 这个会在结束时开始时第一次进入pipelines.py进入，之后不再进入</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        self.client.close()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">    def process_item(self, item, spider):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># print("pipelines.py",item,spider)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 1、测试spider是什么?,结果发现spdier就是我们爬虫程序，因为一个项目里面可以有多个爬虫程序，</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># print("pipelines.py",dir(spider)) # 打印出spider含有的属性和方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># ['allowed_domains', 'close', 'crawler', 'custom_settings', 'from_crawler', 'handles_request', 'log', 'logger', 'make_requests_from_url', 'name', 'parse', 'set_crawler', 'settings', 'start_requests', 'start_urls', 'update_settings']</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># print("pipelines.py", spider.name)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 发现我的爬虫程序的name是唯一的，如果一个项目有多个爬虫程序时，可以根据name进行分别进行建立不同的方法处理（比如不同的爬虫数据，需要存入不同的数据库中，或者需要的字段还不一致）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># pipelines.py scrapyd</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 2、处理返回的数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># print("process_item", item, spider)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># print("type", type(item))</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># 储存到数据库</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        print(<span class="string">"准备保存到数据库"</span>,item)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">        self.collection.save(dict(item))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> item</span></pre></td></tr></table></figure>

<p>打印出的页面显示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009141336984.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>

<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009141550860.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>


<p>数据MongoDB存入成功：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009142539895.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>


<h2 id="6、我刚学scrapy对itmes模块的一个疑问："><a href="#6、我刚学scrapy对itmes模块的一个疑问：" class="headerlink" title="6、我刚学scrapy对itmes模块的一个疑问："></a>6、我刚学scrapy对itmes模块的一个疑问：</h2><p>不知道刚学scrapy时有没有和我一样，有这样一个疑问，爬虫程序中，我不继承items中的ScrapydCnItem类，直接用一个字典代替，其实，我的理解，如果不涉及过的爬虫数据类型保存，是一样的，但是如果涉及过多的类型数据保存，会影响数据的混乱保存。比如进入管道中的，一个程序中，我就有好几种数据分别保存到不同的数据库，这时，继承items中的dict数据进入管道pipelines.py就可以用</p>
<p><strong>isinstance(item,ScrapydCnItem)</strong></p>
<p>返回的是True和False，进行保存自己需要的数据类型，到对应的需求数据库中。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009144127552.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>
<p>scrapy.Item进入源文件是继承一个dict类：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009144411822.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>
<p>scrapy.Field() 进入源文件也是继承一个dict</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009144439116.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>

<p>爬虫文件中，我测试发现定义一个字典返回的管道数据中储存，结果和定义的items的类ScrapydCnItem最后进入管道的结果是一样的，但是类型不一样，最后我找到一个可以理解这定义这个items中ScrapydCnItem类dict的原因：</p>
<h3 id="1、我改为item"><a href="#1、我改为item" class="headerlink" title="1、我改为item={}"></a>1、我改为item={}</h3><p>爬虫spider/scrapyd.py中,改为item = {}：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009143343474.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>
<p>pipelines.py中打印到处item类型，和print(“isinstance”,isinstance(item,ScrapydCnItem))</p>
<p>其中这个ScrapydCnItem类，是items.py中的目标数据字段的类。<br>from scrapyd_cn.items import ScrapydCnItem</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/2019100914554922.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>
<p>上面的打印结果：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/2019100915005359.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>

<h3 id="2、我改为：item-ScrapydCnItem"><a href="#2、我改为：item-ScrapydCnItem" class="headerlink" title="2、我改为：item = ScrapydCnItem()"></a>2、我改为：item = ScrapydCnItem()</h3><p>爬虫spider/scrapyd.py中</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009150136151.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>

<p>pipelines.py中，用来打印出我的疑问</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009150204964.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>
<p>打印出管道中的结果：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/imgs/20191009150314529.png" alt="在这里插入图片描述" title="">
                </div>
                <div class="image-caption">在这里插入图片描述</div>
            </figure>

<h3 id="3、对比，得出我认为的结论："><a href="#3、对比，得出我认为的结论：" class="headerlink" title="3、对比，得出我认为的结论："></a>3、对比，得出我认为的结论：</h3><p>通过上面俩个打印出的结论，我也就自己给出了我的疑问答案，那就是，items的文件，在爬虫程序中继承过来的字段类型，看着是字典，但是可以根据这个进行不同的数据类型（根据继续的items中的类来体现），根据isinstance(item,ScrapydCnItem)，ScrapydCnItem要改为你实际需求的数据类名，进行不同数据类型分开保存。</p>
<p>这也就是我自己对这个items中的定义目标数据字段的理解。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2019-12-02T07:00:37.349Z" itemprop="dateUpdated">2019-12-02 15:00:37</time>
</span><br>


        
        这里可以写作者留言，标签和 hexo 中所有变量及辅助函数等均可调用，示例：<a href="/2019/10/09/b6288208a9fabc0a558dbcfc23223334/" target="_blank" rel="external">https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/</a>
        
    </div>
    
    <footer>
        <a href="https://zhaojiafu.github.io">
            <img src="/img/zhao.jpg" alt="赵家富">
            赵家富
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" target="_blank" rel="noopener" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scrapy/" rel="tag">scrapy</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/&title=《python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）》 — 赵家富的博客&pic=https://zhaojiafu.github.io/img/zhao.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" target="_blank" rel="noopener" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/&title=《python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）》 — 赵家富的博客&source=感觉无从下手的时候，问自己“不难，要你干啥？”" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）》 — 赵家富的博客&url=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/&via=https://zhaojiafu.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" target="_blank" rel="noopener" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/10/10/b06dadf2dcd5da45e4c9d7f1359d8005/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">python爬虫之scrapy 框架学习复习整理四--验证发送请求时携带cookies的4种有效性方法</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/10/09/526b19c90f74051ef7d7d2d6a7e4e2fb/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">python爬虫之scrapy 框架学习复习整理一--最基本入门的知识点</h4>
      </a>
    </div>
  
</nav>



    

















<section class="comments" id="comments">
    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>
        var id = location.pathname
        if (location.pathname.length > 50) {
          id = location.pathname.replace(/\/\d+\/\d+\/\d+\//, '').replace('/', '').substring(0, 50)
        }
        const gitalk = new Gitalk({
          clientID: '388d0a78cda30f118f86',
          clientSecret: '2c64b34cc20f23d768dffff64acb8c2ee02c3bee',
          repo: 'zjf_gittalk1',
          owner: 'zhaojiafu',
          admin: ['zhaojiafu'],
          id: id,      // Ensure uniqueness and length less than 50
          title: document.title.split('|')[0],
          distractionFreeMode: false  // Facebook-like distraction free mode
        })

        gitalk.render('gitalk-container')
    </script>
</section>




</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;" target="_blank" rel="noopener"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
            <span>博客内容遵循 <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=0 height=0 src="http://music.163.com/song/media/outer/url?id=1369798757&auto=1"></iframe>

    </div>
    <div class="bottom">
        <p><span>赵家富 &copy; 2019</span>
            <span>
                

                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" target="_blank" rel="noopener" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/&title=《python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）》 — 赵家富的博客&pic=https://zhaojiafu.github.io/img/zhao.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" target="_blank" rel="noopener" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/&title=《python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）》 — 赵家富的博客&source=感觉无从下手的时候，问自己“不难，要你干啥？”" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《python爬虫之scrapy 框架学习复习整理二--scrapy.Request（自己提取url再发送请求）》 — 赵家富的博客&url=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/&via=https://zhaojiafu.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://zhaojiafu.github.io/2019/10/09/b6288208a9fabc0a558dbcfc23223334/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;" target="_blank" rel="noopener"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACqElEQVR42u3aQXLCQAwEQP7/aXKlKrHRSCvCoX2iKOKofWDFSI9H+Xq+XK/v1D/5+/2r+1y9c/UfD1x4eHh4g9KvSnkWrqs7V9j3j+z+bm8qwcPDw1vjTYrrvX8Fq3/mvmY8PDy87+HdsyuYeutcoeLh4eF9Py8trt5S10MKPDw8vG/g9Vre+/vMk9WPZi14eHh4WeJamiJ9z+uV+R4eHh7eeKqeBgr1ZYL0kOhVi4eHh7fBS4OG9PV8fJWOyvDw8PA+yZt3pGlD3FtNiENhPDw8vDVeb7g1B/cAQUiBh4eHt8CLh/FnW95ytJEO1fDw8PD2eJO4Nl0v6IW86Viu9MsADw8Pr8XrNcqnVqbShar4xwAeHh7eGq/+Z5PQtvdFXzlg3px4eHh4eEd59bF9uiIwL2g+PMPDw8Pb4E2a17Qp7zXxk297PDw8vD3epLVtDvIHCwrBeAwPDw9vgZfuF/S+viuFzgPlx6QIPDw8vHINE0baOk9a6mZYjIeHh7fAO9sWzxvu+sJBsJqAh4eHt8A7u0YwiXEr/XB9DIaHh4e3x6sPwM7uMlUOj8mjxMPDw9vgpU1w2hzPP59GIW9WB/Dw8PDWeOksKf7HYbSRtuZ4eHh4n+RNAtNKU95bDliJbvHw8PBavDRomIy+4rIGjwwPDw9vjzePWdMgI23Z02Dij50yPDw8vKO8SojQW7pKS6kHuMcWCPDw8PBavGd4zRvuSrmV9rpUDx4eHt4Cb3LrevubLlT1Qtv6QYKHh4c3520Mn+otdf0ASFttPDw8vG1e/Ud+L+RNB2NpozxaHcDDw8Nb46VjrfqhUj+c4nUEPDw8vH/lpUdCJWioN8rpoYKHh4e3zTv7pdxb4UofVukx4eHh4S3w5lnoZE0qPWZ6IQgeHh7eId4PVyBpzirIybgAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '死鬼去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



<script src="/node_modules/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"node_modules/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/node_modules/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
